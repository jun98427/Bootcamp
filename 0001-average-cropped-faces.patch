From 4f56b82b01acc435aac59ba5f1a53e8764ac363f Mon Sep 17 00:00:00 2001
From: heejun <jun98427@naver.com>
Date: Wed, 19 Mar 2025 12:00:47 +0900
Subject: [PATCH 1/1] average cropped faces

---
 application/Inference.py  |  4 +++-
 application/Processing.py | 39 ++++++++++++++++++++-------------------
 application/main.py       | 12 +++++++-----
 3 files changed, 30 insertions(+), 25 deletions(-)

diff --git a/application/Inference.py b/application/Inference.py
index 58ac534..00457a4 100644
--- a/application/Inference.py
+++ b/application/Inference.py
@@ -95,7 +95,9 @@ class Inference:
 
         for l, v in zip(["리더십", "매력", "신뢰도", "피지컬", "예술", "지능"], skills):
             # default_point = random.randint(40, 60)
-            self.inf_skills[l] = min(v*450, 100)
+            v *= 450
+            v = min(v, 100)
+            self.inf_skills[l] = max(v, 40)
 
     def calc_values(self):
         self.careers = {
diff --git a/application/Processing.py b/application/Processing.py
index aab8725..4e1b779 100644
--- a/application/Processing.py
+++ b/application/Processing.py
@@ -15,22 +15,19 @@ net.load_model(bin_path)
 
 class Processing :
     def __init__(self):
-        self.cropped_face = None
         self. result_list = [-1,-1,-1,-1,-1,-1]
-        self.last_cropped_face = None
         
     def detect_face(self,frame):
         """ 현재 프레임을 캡처하여 저장 """
         self.cropped_face = None
         x1, y1, x2, y2 = 0,0,0,0
-        # results = ncnn_model(frame, max_det=1, imgsz=128, conf=0.3)
+
         results = ncnn_model.predict(frame, max_det=1, imgsz=128, conf=0.3, verbose=False)
         for i, r in enumerate(results):
             for box in r.boxes.xyxy:  # bounding box (x1, y1, x2, y2)
                 x1, y1, x2, y2 = map(int, box)
                 upper_margin = 0
                 y1 = max(0, y1-upper_margin)
-                self.cropped_face = frame[y1:y2, x1:x2]
         return x1, y1, x2, y2
 
     def softmax_with_temperature(self, logits, temperature=1.0):
@@ -44,27 +41,31 @@ class Processing :
         return exp_values / np.sum(exp_values)
     
     # 캡쳐된 jpg 파일을 받아서 classification 하고 6개짜리 리스트를 출력
-    def classification(self):
-        output = [0.11, 0.175, 0.1675, 0.15, 0.30, 0.12]
+    def classification(self, cropped_frames):
+        output = np.array([0.11, 0.175, 0.1675, 0.15, 0.30, 0.12])
+
+        arr = np.zeros(6)
+        count = 0
 
-        if not np.array_equal(self.cropped_face, self.last_cropped_face):
-            # cv2.imread("captured_frame.jpg", jpg_file)
-            self.result_list = [-1,-1,-1,-1,-1,-1]
+        for cropped_frame in cropped_frames:
             ex = net.create_extractor()
-            cropped_face_resized = cv2.resize(self.cropped_face, (128, 128), interpolation=cv2.INTER_LINEAR)
-            # RGB to NCNN Mat
-            cropped_face_ncnn = ncnn.Mat.from_pixels(cropped_face_resized, ncnn.Mat.PixelType.PIXEL_RGB, 128, 128)
-            # Normalize
+            cropped_frame_resized = cv2.resize(cropped_frame, (128, 128), interpolation=cv2.INTER_LINEAR)
+            cropped_frame_ncnn = ncnn.Mat.from_pixels(cropped_frame_resized, ncnn.Mat.PixelType.PIXEL_RGB, 128, 128)
             mean_vals = [0.5 * 255, 0.5 * 255, 0.5 * 255] 
             norm_vals = [1 / (0.5 * 255), 1 / (0.5 * 255), 1 / (0.5 * 255)]
-            cropped_face_ncnn.substract_mean_normalize(mean_vals, norm_vals)
+            cropped_frame_ncnn.substract_mean_normalize(mean_vals, norm_vals)
             # NCNN input
-            ex.input("in0", cropped_face_ncnn)
-
-            ret, output = ex.extract("out1")
+            ex.input("in0", cropped_frame_ncnn)
+            ret, output = ex.extract("out1") 
             output_np = np.array(output)
             temperature = 3.5
             output = self.softmax_with_temperature(output_np, temperature)
-            
-            self.last_cropped_face = self.cropped_face
+            print("output ", output_np)
+            count += 1
+            arr += output
+
+        if count != 0:
+            arr /= count
+            return arr
+
         return output
\ No newline at end of file
diff --git a/application/main.py b/application/main.py
index 132fdfd..e833461 100644
--- a/application/main.py
+++ b/application/main.py
@@ -190,6 +190,7 @@ class CameraApp(QWidget):
         self.countdown_timer = QTimer(self)
         self.countdown_timer.timeout.connect(self.update_countdown)
         self.frames = []
+        self.cropped_frames = []
         self.line_color = 'white'
 
         self.greenCnt = 0
@@ -248,7 +249,6 @@ class CameraApp(QWidget):
         self.countdown = 2
         self.greenCnt = 0
         self.redCnt = 0
-        self.frames = []
         self.line_color = 'white'
         self.capture_data = False
         self.result_type = None
@@ -320,7 +320,7 @@ class CameraApp(QWidget):
                         if self.greenCnt >= 10 :
                             self.line_color = 'green'
                             self.frames.append(frame)
-                            self.frames.append(cropped_face)
+                            self.cropped_frames.append(cropped_face)
                             # self.cropped_face = cv2.cvtColor(self.cropped_face, cv2.COLOR_BGR2RGB)
                             # self.face_thread.set_frame(frame=frame)
                             # self.cap.capture_face(self.cropped_face)
@@ -721,12 +721,14 @@ class CameraApp(QWidget):
         event.accept()
 
     def start_request(self):
-        inf = infer.Inference(self.pro.classification())
+        inf = infer.Inference(self.pro.classification(self.cropped_frames))
         self.skills = inf.get_skills()  
         self.careers, self.animals, self.careers_scores, self.animals_scores = inf.infer_careers()
         self.result_info, self.careers_info, self.animals_info, self.celeb_info = inf.get_formats()
-
-        self.face_thread.set_frame(frames=self.frames)
+        frames = self.frames + self.cropped_frames
+        self.face_thread.set_frame(frames=frames)
+        self.frames = []
+        self.cropped_frames = []
 
 
     def handle_api_response(self, data):
-- 
2.39.5

